Excitement over humanities computing, or the so-called "Digital Humanities", has foregrounded questions about how humanistic scholars construct our research frameworks.
In particular, this enthusiasm has [renewed anxiety about whether and how humanities subjects should be represented as computable, quantifiable data][posner].
Whether we work with digital tools or with more "traditional" methods, we are pressured to partition the qualitative and quantitative aspects of our work.

However, this is a fallacy. Rather than an immutable divide, the distinctions between qualitative and quantitative models are more akin to the [wave-particle duality of light][duality], a model of coexistence where the frame of either "wave" or "particle" is determined by the perspective and purpose of the researcher within a given instant.

The following case studies suggest how blurry the qualitative/quantitative boundary truly is in humanities research, and how an iterative process of models, measures, and data can help humanist scholars reckon with the many quantitative assumptions interweaved with our work. It is this highly-recursive process that we argue lies at the heart of any practice in humanistic experimental design. We will argue that scholars ought to embrace qualitative/quantitative iteration not as a turn towards simplistic determinism, but rather a continual striving towards clarification of processes.

[posner]: http://miriamposner.com/blog/humanities-data-a-necessary-contradiction/

[duality]: https://en.wikipedia.org/wiki/Wave%E2%80%93particle_duality
